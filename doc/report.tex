\documentclass[14pt,a4paper, oneside]{extarticle}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{indentfirst}
\usepackage[hidelinks]{hyperref}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{top=2cm, bottom=2cm, left=2.5cm, right=2.5cm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{setspace}
\usepackage{float}


\onehalfspacing

\begin{document}

\thispagestyle{empty}


\begin{center}
    \textbf{Московский Авиационный Институт\\
    (Национальный Исследовательский Университет)}
    
    \vspace{2cm}
    
    \textbf{Факультет информационных технологий и прикладной математики\\
    Кафедра вычислительной математики и программирования}
    
    \vspace{3cm}
    
    \textbf{Лабораторные работы по курсу\\
    «Информационный поиск»}
\end{center}

\vspace{3cm}

\begin{flushright}
\begin{minipage}{0.52\textwidth}
\raggedright
Студент: Юрков Е.Ю.\\[0.7em]
Группа: М8О-412Б-22\\[0.7em]
Преподаватель: \underline{Кухтичев А. А.}\\[0.7em]
Оценка: \underline{\hspace{5.55cm}}\\[0.7em]
Дата: \underline{\hspace{6.05cm}}\\[0.7em]
Подпись: \underline{\hspace{5.3cm}}
\end{minipage}
\end{flushright}

\vfill

\begin{center}
    \textbf{Москва, 2025}
\end{center}

\newpage

\begin{center}
    \textbf{Содержание}
\end{center}

\vspace{1cm}

\begin{flushleft}
\noindent
Цель работы \dotfill \pageref{sec:1}\\[0.7em]
Описание данных \dotfill \pageref{sec:2}\\[0.7em]
Обкачка документов \dotfill \pageref{sec:3}\\[0.7em]
Предобработка текста \dotfill \pageref{sec:4}\\[0.7em]
Индекс и сжатие \dotfill \pageref{sec:5}\\[0.7em]
Обработка запросов \dotfill \pageref{sec:6}\\[0.7em]
Вывод \dotfill \pageref{sec:8}\\[0.7em]
\end{flushleft}

\newpage

\section*{Цель работы}
\label{sec:1}

\begin{itemize}
    \item Добыча корпуса документов
    \item Создание поискового робота
    \item Реализовать поисковый движок со следующими компонентами:
    \begin{itemize}
        \item Токенизация
        \item Лемматизация
        \item Булев индекс
        \item Булев поиск
        \item Цитатный поиск
        \item Сжатие индекса
        \item Ускорение, прыжки по индексу
        \item Ранжирование (TF-IDF)
        \item Автотесты
        \item Построение сниппетов
    \end{itemize}
    \item Рассчитать график закона Ципфа для выбранного корпуса документов.
\end{itemize}

\newpage

\section*{Описание данных}
\label{sec:2}

Для корпуса документов были выбраны следующие сайты: \textit{habr.com}, \textit{www.geeksforgeeks.com}.
Они содержат различные статьи из темы ИТ на русском и английском языках.

Для обхода страниц сайтов использовались sitemap.xml:
\url{https://habr.com/sitemap.xml} и
\url{https://www.geeksforgeeks.org/sitemap_index_new.xml}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{habr.png}
    \caption{Пример страницы с habr.com}
    \label{fig:habr}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{geeks.png}
    \caption{Пример страницы с www.geeksforgeeks.com}
    \label{fig:geeks}
\end{figure}

Для тестирования поискового движка было скачано и обработано 50000 статей.

\section*{Обкачка документов}
\label{sec:3}

Поисковый робот был реализован на языке \texttt{Python}.

Был реализован класс \texttt{UrlFetcher}, который совершает обход sitemap
и предоставляет url сайтов для обкачки в асинхронную очередь.
Класс \texttt{PageDownloader} брал url из очереди и скачивал страницы.
Обращение к страницам в сети интернет производилось с помощью библиотеки \texttt{aiohttp}.
Для парсинга статьи из html использовалась библиотека \texttt{BeautifulSoup}.
Также был реализован класс \texttt{StorageManager}, который сохранял скачанные страницы
в базу данных \textit{Mongo}. Для взаимодействия с \textit{MongoDB}
использовалась библиотека \texttt{pymongo}.

Итоговый документ после скачивания в базе данных имел следующие поля:
\begin{itemize}
    \item \_id - ObjectID, генерируемый \textit{MongoDB};
    \item url - ссылка на страницу в интернете;
    \item site - источник, с которого взята страница (\textit{habr.com} или \textit{www.geeksforgeeks.com});
    \item html - сырой html страницы, который использовался для парсинга.
    \item text - заголовок и текст извлечённый парсером (возможно пустой);
    \item created\_at - время скачивания страницы;
    \item doc\_id - идентификатор документа типа int32, который используется при построении индекса.
\end{itemize}

Также поисковый робот сохранял в отдельную коллекцию базы данных
информацию об url последней скачанной страницы
для каждого сайта, чтобы при повторном запуске начинать обкачку с неё и не повторяться.

\section*{Предобработка текста}
\label{sec:4}

Сам поисквый движок был написан на языке C++.
Первой реалзованной библиотекой была библиотека \texttt{linguistics}.
В ней содеражатся классы \texttt{Tokenizer}, \texttt{Lemmatizer} и \texttt{Preprocessor},
который объединяет в себе предыдущие два.

Для токенизации есть несколько правил: для обработки слов, чисел и аббревиатур.
Они определяют удовлетворяет ли текст,
начинающийся с заданной позиции правилу и отделяют токен от текста.
Разделение на правила нужно, из-за того,
что не всегда правильно отделять токен по пробелам и знакам препинания.
Например, аббревиатуры могут быть записаны подряд,
а могут быть разделены точками хотя являются целым словом: СССР или С.С.С.Р.,
числа могут быть дробными и содержать запятую или точку: 0.25 или 0,25.

Для лемматизации было реализовано два класса: \texttt{MockLemmatizer},
который не производил лемматизацию, но использовался в тестах для создания \texttt{Preprocessor},
и \texttt{DictLemmatizer}, который производил словарную лемматизацию.
Для словарной лемматизации был скачан словарь лемм \textit{OpenCorpora} для русских слов и
словарь \textit{UniMorph} для английских слов.
Для слов, которые не нашлись в словаре использовался простой алгоритм стемминга,
который обрезает окончания слов. Для скачивания словарей написан скрипт \texttt{scripts/build\_dicts.py}.

Для токенизации и стемминга были написаны тесты с использованием библиотеки \textit{GoogleTest}.

\newpage

\section*{Индекс и сжатие}
\label{sec:5}

Основная библиотека поискового движка - \texttt{engine}.
В ней реализован индекс и обработчики запросов.

Индекс представлен классом \texttt{InvertedIndex}.
Он хранит списки постингов соответствующие каждому слову.
Списки хранятся в классе \texttt{HashTable}, который представляет собой хеш таблицу,
которая в качестве ключа использует \texttt{std::string},
а в качестве значения - \texttt{PostingList}.

Для ранжирования в индексе для каждого документа добавляется количество вхождений слова (tf).
А для поддержания цитатных запросов нужно хранить ещё и координаты этого слова в документе.
Поэтому в \texttt{PostingList} хранятся следующие структуры:
\begin{itemize}
    \item \texttt{doc\_buffer}: \texttt{[(doc\_id, coord\_pos), \dots]};
    \item \texttt{coord\_buffer}: \texttt{[(tf, [pos\_0, pos\_1, \dots]), \dots]}.
\end{itemize}
где \texttt{coord\_pos} — позиция начала координатного списка в массиве
\texttt{coord\_buffer},
\texttt{tf} — размер массива координат или количество вхождений слова в документ,
а \texttt{pos\_i} — позиции слова в документе.

Для сжатия \texttt{PostingList} был реализован класс \texttt{CompressedPostingList}.
В нём данные doc\_buffer и coord\_buffer сжаты с помощью V-Byte Encoding.
После сжатие усложнился доступ к элементам списка, поэтому для булевого поиска
и ранжированного поиска были реализованы классы итераторов:
\texttt{DocIterator} для получения doc\_id, tf и создания итератора по координатам
и \texttt{CoordIterator} для итерации по позициям в документе (для цитатного поиска).
Для ещё большего сжатия вместо самих doc\_id храняться разности соседних документов,
что также позволяет уменьшить размер doc\_id и увеличить пользу V-Byte Encoding.
Аналогичный подход применяется и к координатам в документе.

Для ускоренной итерации при пересечении двух \texttt{PostingList}
используется \texttt{skip\_buffer}, который хранит указатели для прыжков по индексу.
Они хранят позиции части документов в \texttt{doc\_buffer}.
Чтобы таблица прыжков не занимала слишком много памяти (так как к ней не применяется сжатие)
и достаточно ускоряла обход списка документов размеры прыжка определяются по следующей
формуле: $ skip\_size = \sqrt{N} $, где N - это общее количество документов в списке.

Для расчёта TF-IDF в классе \texttt{InvertedIndex} также храняться
длины каждого документа, они используются для косинусной нормализации.

Чтобы не терять индекс после его построения был написан класс
\texttt{FileIndexStorage}. Он позволяет скачивать и загружать
индекс из бинарного файла.

На основе данных индекса была рассчитана зависимость частоты
слов от их порядка в отсортированном по частоте списке.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{zipfs_law.png}
    \caption{График закона Ципфа}
    \label{fig:zipf}
\end{figure}

Закон Ципфа описывает частоту появления элементов в ранжированных списках.
Он утверждает, что во многих наборах данных естественного языка
частота элемента обратно пропорциональна его рангу.

Если упорядочить слова в тексте по убыванию частоты, то:
\[
P(r) ~ \frac{1}{r^\alpha}
\]
где \( P(r) \) - частота элемента с рангом \( r \),
\( r \) - ранг элемента,
\( \alpha \) - коэффициент, близкий к 1 для многих естественных распределений.

Визуальный анализ графика в двойных логарифмических координатах показывает выраженную линейную зависимость.
Это подтверждает, что распределение частот терминов в корпусе документов
соответствует степенному закону и, в частности, закону Ципфа.

\section*{Обработка запросов}
\label{sec:6}

У поискового движка есть два режима: булевый поиск и ранжированный.
Булевый используется при запуске с флагом \texttt{--boolean}.
Ранжированный используется по умолчанию.
В движке реализована поддержка цитатных (фразовых) запросов, которые учитывают
относительный порядок слов при поиске. Для их использования нужно брать
искомую фразу в кавычки.

Класс обработки булевых запросов называется \texttt{BoolQuery}, у него есть
два основных метода: \texttt{Parse} и \texttt{Execute}.
При парсинге булевого запроса строится дерево, где каждый \texttt{ASTNode}
имеет один из следующих типов: Term, Phrase, And или Or.
При исполнении запроса операции And и Or выполняются с помощью соответствующих
методов \texttt{CompressedPostingList},
которые используют прыжки по индексу для ускорения операций.

Класс обработки ранжированных запросов \texttt{RankedQuery} имеет тот
же интерфейс, что и \texttt{BoolQuery}. Для ранжирования используется TF-IDF.

В данной реализации используется векторная модель поиска с TF--IDF взвешиванием и косинусной мерой сходства между запросом и документами.

\paragraph{TF (term frequency).}
Для терма \( t \) применяется логарифмически сглаженная частота:
\[
\mathrm{tf}(t, d) =
\begin{cases}
1 + \log f_{t,d}, & f_{t,d} > 0, \\
0, & f_{t,d} = 0,
\end{cases}
\]
где \( f_{t,d} \) — число вхождений терма \( t \) в документе \( d \).
Аналогично определяется частота терма в запросе \( q \):
\[
\mathrm{tf}(t, q) = 1 + \log f_{t,q}.
\]

\paragraph{IDF (inverse document frequency).}
Используется сглаженная версия обратной документной частоты:
\[
\mathrm{idf}(t) = \log \frac{1 + N}{1 + \mathrm{df}(t)} + 1,
\]
где \( N \) — общее число документов в коллекции, а \( \mathrm{df}(t) \) —
количество документов, содержащих терм \( t \).
Сглаживание предотвращает деление на ноль и уменьшает влияние редких выбросов.

\paragraph{Веса термов.}
Вес терма \( t \) в документе и в запросе определяется как:
\[
w_{t,d} = \mathrm{tf}(t,d) \cdot \mathrm{idf}(t),
\qquad
w_{t,q} = \mathrm{tf}(t,q) \cdot \mathrm{idf}(t).
\]

\paragraph{Скалярное произведение.}
Ненормализованная оценка релевантности документа \( d \) запросу \( q \)
вычисляется как скалярное произведение TF--IDF векторов:
\[
\mathrm{score}(d, q) =
\sum_{t \in q \cap d} w_{t,d} \cdot w_{t,q}.
\]

\paragraph{Косинусная нормализация.}
Для устранения влияния длины документа и запроса применяется нормализация:
\[
\lVert q \rVert = \sqrt{\sum_{t \in q} w_{t,q}^2},
\qquad
\lVert d \rVert = \sqrt{\sum_{t \in d} w_{t,d}^2}.
\]
Итоговый скор вычисляется по формуле косинусного сходства:
\[
\mathrm{score}_{\text{norm}}(d, q) =
\frac{\mathrm{score}(d, q)}{\lVert d \rVert \cdot \lVert q \rVert}.
\]

Документы сортируются по убыванию значения
\( \mathrm{score}_{\text{norm}}(d, q) \).

\newpage

\section*{Вывод}
\label{sec:8}

В ходе выполнения лабораторных работ по предмету «Информационный поиск»
были последовательно изучены и реализованы ключевые этапы построения поисковой системы,
начиная с добычи корпуса документов и разработки поискового робота
и заканчивая созданием полноценного поискового движка.
В рамках проекта были рассмотрены базовые и продвинутые методы обработки текстов,
включая токенизацию и лемматизацию, а также построение булева индекса
и реализацию различных видов поиска, таких как булев и цитатный.

Особое внимание было уделено вопросам эффективности хранения и обработки данных:
реализованы механизмы сжатия индекса, ускорения поиска за счёт прыжков по индексным спискам
и ранжирования документов на основе метрики TF-IDF.
Дополнительно была выполнена разработка автотестов
для проверки корректности работы поискового движка
и реализовано построение сниппетов, повышающих удобство представления результатов поиска.
Анализ распределения частот слов в корпусе документов с помощью графика закона Ципфа
позволил на практике изучить статистические свойства
естественного языка и их влияние на задачи информационного поиска.

Выполнение данного проекта позволило получить целостное представление
о внутреннем устройстве поисковых систем и основных алгоритмах,
применяемых в информационном поиске.
Полученные навыки могут быть полезны при разработке поисковых сервисов,
систем анализа текстов, рекомендательных систем и других приложений,
работающих с большими объёмами текстовой информации.

\end{document}

